\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{diagbox}
%
\begin{document}
	\hyphenation{}
	
	\title{Proyecto Final de Sistemas de Recuperaci\'on de Informaci\'on}
	
	\author{Rocio Ortiz Gancedo \and Carlos Toledo Silva}
	
	\institute{Universidad de La Habana, Cuba}
	
	\titlerunning{Proyecto Final de Sistemas de Recuperaci\'on de Informaci\'on}
	
	\maketitle
	
	\begin{abstract}
		
	\end{abstract}

	\section{Introducci\'on}

	\section{Dise\~{n}o del sistema}
	
	\subsection{Recordatorio de las caracter\'isticas del modelo vectorial}
	
	En el modelo vectorial, el peso $w_{i,j}$ asociado al par $(t_i,d_j)$ (siendo $t_i$ el t\'ermino $i$ y $d_j$ el documento $j$) es positivo y no binario. A su vez, los t\'erminos en la consulta est\'an ponderados. Sea $w_{i,q}$ el peso asociado al par $(t_i,q)$ (siendo $q$ una consulta), donde $w_{i,q}\geq 0$. Entonces, el vector consulta $q$ se define como $\overrightarrow{q}=(w_{1q},w_{2q},...,w_{nq})$ donde $n$ es la cantidad total de t\'erminos indexados en el sistema. El vector de un documento $d_j$ se representa por $\overrightarrow{d_j}=(w_{1j},w_{2j},...,w_{nj})$.
	
	La correlaci\'on se calcula utilizando el coseno del \'angulo comprendido entre los vectores documentos $dj$ y la consulta $q$.
	
	\begin{equation}
		sim(d_j,q)=\frac{\overrightarrow{d_j}\cdot\overrightarrow{q}}{|\overrightarrow{d_j}|\cdot|\overrightarrow{q}|}
	\end{equation}

	\begin{equation}	
		sim(d_j,q)=\frac{\sum_{i=1}^{n}w_{i,j}\cdot w_{i,q}}{\sqrt{\sum_{i=1}^{n}w_{i,j}^2}\cdot\sqrt{\sum_{i=1}^{n}w_{i,q}^2}}
	\end{equation}  

	Sea $freq_{i,j}$ la frecuencia del t\'ermino $t_i$ en el documento $d_j$. Entonces, la frecuencia normalizada $tf_{i,j}$ del t\'ermino $t_i$ en el documento $d_j$ est\'a dada por:
	
	\begin{equation}
		tf_{i,j}=\frac{freq_{i,j}}{max_lfreq_{l,j}}
	\end{equation}

	donde el m\'aximo se calcula sobre todos los t\'erminos del documento $d_j$. Si el t\'ermino $t_i$ no aparece en el documento $d_j$ entonces $tf_{i,j}=0$.
	
	Sea $N$ la cantidad total de documentos en el sistema y $n_i$ la cantidad de documentos en los que aparece el t\'ermino $t_i$. La frecuencia de ocurrencia de un t\'ermino $t_i$ dentro de todos los documentos de la colecci\'on $idf_i$ est\'a dada por:
	
	\begin{equation}
		idf_i=\log \frac{N}{n_i}
	\end{equation}

	El peso del t\'ermino $t_i$ en el documento $d_j$ est\'a dado por:
	
	\begin{equation}
		w_{i,j}=tf_{i,j}\cdot idf_i
	\end{equation}

	El c\'alculo de los pesos en la consulta $q$ se hace de la siguiente forma:
	
	\begin{equation}
		w_{i,q}=\left\{\begin{array}{c}
			0,~si~freq_{i,q}=0\\
			\left(a+(1-a)\frac{freq_{i,q}}{max_lfreq_{l,q}}\right)\cdot\log \frac{N}{n_i},~en~otro~caso
		\end{array}\right. 
	\end{equation}

	donde $freq_{i,q}$ es la frecuencia del t\'ermino $t_i$ en el texto de la consulta $q$. El t\'ermino $a$ es de suavizado y permite amortiguar la contribuci\'on de la frecuencia del t\'ermino, toma un valor en 0 y 1. Los valores m\'as usados son 0.4 y 0.5. 
	
	\subsection{\textquestiondown Por qu\'e seleccionamos el modelo vectorial?}
	
	Se seleccion\'o el modelo vectorial primeramente por la amplia cantidad de elementos impartidos durante el curso sobre este modelo. Adem\'as este presenta las siguientes ventajas:
	
	\begin{itemize}
		\item El esquema de ponderaci\'on $tf-idf$ para los documentos mejora el rendimiento de la recuperaci\'on.
		\item La estrategia de coincidencia parcial permite la recuperaci\'on de documentos que se aproximen a los requerimientos de la consulta.
		\item La f\'ormula del coseno ordena los documentos de acuerdo al grado de similitud.
	\end{itemize}

	Adem\'as de estas ventajas tambi\'en cabe destacar la muy buena posibilidad de retroalimentaci\'on que admite este modelo.
	
	\subsection{Ideas interesantes}
	
	\subsubsection{Usar diccionarios para representar a los vectores y las consultas}
	Una idea interesante que utilizamos para representar lo que son los vectores en la teor\'ia como los vectores de los t\'erminos y de pesos tanto de los documentos como de las consultas, en lugar de como vectores los implementamos como diccionarios, tal que la clave es el t\'ermino $i$ (preprocesado) y la clave seg\'un cual sea el diccionario, ser\'ia la frecuencia o el peso del t\'ermino en un documento o consulta en espec\'ifico. 
	
	Esto lo hacemos debido a la gran cantidad de t\'erminos que pudieran haber una colecci\'on grande de t\'erminos y representar cada documento mediante un vector de longitud igual a la cantidad total de t\'erminos distintos ser\'ia altamente costoso en memoria y en tiempo de ejecuci\'on. Adem\'as de la gran probabilidad de que la matriz conformada por los vectores de frecuencia de los t\'erminos y en consecuencia la matriz formada por los vectores de pesos de los t\'erminos en los documentos sean muy esparcidas, pues lo m\'as probable, si se tiene una colecci\'on grande de documentos, es que un t\'ermino $t_i$, si es relevante, aparezca en una cantidad muy inferior de documentos con respecto al total de los mismos.
	
	Adem\'as esto lo podemos hacer debido a que un t\'ermino que no aparezca en un documento, dado que su frecuencia es cero y por como se calculan los pesos y la similitud entre un documento y una consulta, no afecta para nada el c\'alculo de estos par\'ametros. Veamos esto r\'apidamente:
	
	Por (2) tenemos en el numerador una sumatoria donde el t\'ermino $i$ de la sumatoria es 0 si $w_{i,j}=0 \lor w_{i,q}=0$. Por (5) tenemos que $w_{i,j}=0$ si $tf_{i,j}=0 \lor idf_i=0$. Luego por (3) tenemos que $tf_{i,j}=0$ si $freq_{i,j}=0$, o sea si el t\'ermino $t_i$ no aparece en el documento $d_j$
	
	Para la consulta, por (6) tenemos que si la $freq_{i,q}=0$ entonces $w_{i,q}=0$.
	
	Por tanto si el t\'ermino $t_i$ no aparece en el documento o no aparece en la consulta, entonces $t_i$ no influye en la sumatoria del numerador.
	
	Pasemos entonces a analizar el denominador. En este tenemos dos sumatorias: una que itera por los cuadrados de los pesos del vector del documento y otra que itera por los cuadrados de los pesos del vector de la consulta. Es evidente que si el peso de $t_i$ en $d_j$ es 0 entonces este no influye en la sumatoria. Lo mismo ocurre si un t\'ermino no aparece en la consulta $q$.
	
	Por tanto llegamos a la conclusi\'on que para calcular la similitud entre una consulta y un documento solo necesitamos los t\'erminos que aparecen en el documento o en la consulta.
	
	 \subsubsection{Guardar vol√∫menes de informaci\'on en .json, sobre todo aquellas que se utilizan mucho} De esta forma se devid\'ia la ejecuci\'on en partes, en vez de hacerlo todo de una vez lo cual se demorar\'ia un tiempo considerable y adem\'as cada vez que se quisiera correr el sistema se estar\'ian haciendo los mismos c\'alculos. Algunos objetos guardados en .json son la representaci\'on de los vectores y las consultas y un diccionario que contiene a cada t\'ermino con su respectivo $idfs$.
	
	\subsubsection{Utilizar un heap para llevar el ranking de similitud de una consulta} Utilizando esta estructura llevamos un orden parcial de las similitudes de una consulta con los documentos en el sistema.
		
	\section{Implementaci\'on del sistema}
	
	\subsection{Preprocesamiento de los documentos}
	
	Las funciones para el preprocesamiento de los documentos de la colecci\'on Cranfield las podemos encontrar en ``cran\_preprocess.py''. Primero tenemos la funci\'on \verb|cran_preprocessing| la cual, a partir de la colecci\'on de documentos, devuelve un diccionario tal que las llaves son los t\'erminos que aparecen en los documentos y el valor asociado a cada t\'ermino es el conjunto de los documentos en los que aparece dicho t\'ermino. Los t\'erminos a su vez son sometidos a un preprocesamiento y para esto utilizamos la librer\'ia \verb|nltk|.
	
	El proceso de tokenizaci\'on se hace mediante la funci\'on \verb|word_tokenize| la cual a partir de un texto(por defecto en ingl\'es) devuelve los diferentes tokens de dicho texto. Luego los tokens son clasificados sint\'acticamente y etiquetados con dicha clasificaci\'on mediante el m\'etodo \verb|pos_tag|. Esto se hace pues para el proceso de ``\textit{lemmatizing}''(llevar las palabras a su ra\'iz gramatical) que es el proceso que viene a continuaci\'on, tener los tokens clasificados mejora el rendimiento de este proceso. Despu\'es de realizado
	el proceso de \textit{lemmatizing}, se chequea si los t\'erminos obtenidos son ``\textit{stopwords}''(palabras que no proveen informaci\'on \'util). Si un t\'ermino $i$ no es una \textit{stopword}, si no est\'a en el diccionario, se a\~{n}ade como llave y se crea un conjunto con el documento actual. Si ya est\'a el t\'ermino en el diccionario entonces se a\~{n}ade al conjunto correspondiente al t\'ermino el documento actual. Como es un conjunto si el documento ya est\'a en el conjunto, este no se agregar\'a. Para saber el documento actual y la cantidad que de documentos que se ha visto simplemente se aumenta la variable \verb|actual_document|.
	
	Debido a que se analizan todos los documentos de una colecci\'on y de cada uno de estos se analizan todos sus t\'erminos, asumiendo que todas las operaciones que se realizan sobre un token se hacen en $O(1)$, llegamos a la conclusi\'on que una llamada a esta funci\'on tiene una complejidad temporal $O(n\cdot m)$; siendo $n$ la cantidad total de documentos y $m$ la cantidad de tokens del documento que mayor cantidad de tokens tiene.
	
	La otra funci\'on que aparece en este archivo es \verb|terms_freq_doc|, la cual devuelve de forma perezosa un diccionario para cada documento de la colecci\'on y un valor entero. El diccionario tiene como llaves los t\'erminos que aparecen en dicho documento y el valor asociado a cada t\'ermino es la frecuencia del t\'ermino en el documento. El n\'umero entero que se devuelve junto con el diccionario es la frecuencia del t\'ermino de mayor frecuencia en el documento. Cada vez que se detecte un documento nuevo, se crea un diccionario y un entero inicializado con 0. El procesamiento de los t\'erminos se hace de forma similar que en el m\'etodo anterior. Por cada t\'ermino $i$ en el documento $j$ se verifica si ya este fue agregado al diccionario. En caso afirmativo se incrementa en 1 su valor asociad y en caso contrario se agreg\'a el t\'ermino al diccionario, asoci\'andole 1 como valor, pues es la primera vez que se detecta. Despu\'es de esto se comprueba si dado este aumento la frecuencia del t\'ermino aument\'o, de tal forma que se hizo mayor que la m\'axima frecuencia registrada detectada hasta el momento para el documento. De ocurrir lo antes planteado se actualiza entonces la m\'axima frecuencia detectada (la variable \verb|max_freq|). Cada vez que se termine de analizar un documento se devuelve el diccionario y el entero antes mencionado.
	
	Como se explic\'o una ejecuci\'on completa de este m\'etodo analiza cada uno de los documentos y sus tokens. Adem\'as realiza una gran cantidad de operaciones similares al anterior y las que tiene diferente con respecto al otro tienen una complejidad temporal despreciable (son $O(1)$). Por tanto llegamos a la conclusi\'on que una ejecuci\'on completa de este m\'etodo es $O(n\cdot m)$, siendo $n$ y $m$ los mismos valores mencionados anteriormente.
	
	\subsection{Representaci\'on de los documentos}
	
	El c\'odigo relacionado a este apartado lo podemos encontrar en ``docs\_representation.py''. Primero veamos como hallar los valores $idf_i$ para los diferentes t\'erminos. Estos valores se calculan utilizando la funci\'on \verb|calculate_idfs| la cual recibe un diccionario que tiene como llaves a los diferentes t\'erminos y el objeto asociado a un t\'ermino $t_i$ es el conjunto de documentos en los que el t\'ermino $t_i$ aparece; adem\'as de un valor entero que indica la cantidad de documentos que hay en el sistema. M\'as concretamente este m\'etodo recibe la salida del m\'etodo \verb|cran_preprocessing| u otro que de una salida similar. Est\'e m\'etodo devuelve un diccionario que tiene como llaves a cada uno de los t\'erminos, y el valor asociado a cada t\'ermino $t_i$ es su respectivo valor $idf_i$. Lo que se hace dentro del m\'etodo es lo siguiente: 

	\begin{itemize}
		\item Crear el diccionario \verb|idfs|.
		\item Por cada t\'ermino en el diccionario de entrada:
		\begin{itemize}
			\item Calcular su $idf_i$ correspondiente y guardar la pareja $t_i$ y $idf_i$ en el diccionario \verb|idfs| como llave y valor respectivamente.
		\end{itemize}
		\item  Retornar el diccionario \verb|idfs|.
	\end{itemize}
	 
	 Es sencillo notar que la complejidad temporal de esta funci\'on es $O(k)$, siendo $k$ el n\'umero total de t\'erminos.
	
	Veamos ahora como calcular los valores $tf_{i,j}$ para cada t\'ermino en un documento. La funci\'on utilizada para esto es \verb|calculate_tfijs| que recibe un objeto iterable, tal que cada elemento del objeto iterable es una tupla $<$diccionario, entero$>$. El diccionario tiene como llaves los t\'erminos que aparecen en un documento $d_j$ y el entero indica la frecuencia del t\'ermino de mayor frecuencia en el documento $d_j$. Est\'e m\'etodo retorna de forma perezosa un diccionario por cada documento $d_j$ que contiene como llave los t\'erminos de dicho documento y para cada t\'ermino su valor asociado es el valor $tf_{i,j}$ correspondiente. La forma de hacer esto es:
	
	\begin{itemize}
		\item Por cada elemento del objeto iterable:
		\begin{itemize}
			\item Crear el diccionario \verb|tfj|
			\item Por cada t\'ermino en el diccionario obtenido del objeto iterable:
			\begin{itemize}
				\item Calcular el $tf_{i,j}$ correspondiente y guardar dicha pareja en el diccionario \verb|tfj|
			\end{itemize}
			\item Retornar \verb|tfj| 
		\end{itemize}
	\end{itemize}
	
	La idea de hacerlo de forma perezosa es no tener en memoria grandes vol\'umenes de informaci\'on que no son necesarios en todo momento. Ya que se analizan los $n$ documentos de la colecci\'on y en cada una la cantidad de t\'erminos es $O(m)$, una ejecuci\'on completa de este m\'etodo es $O(n\cdot m)$.
	
	Como ya vimos como calcular $idf$ y $tf$ a continuaci\'on se explicar\'a como calcular entonces los ``vectores'' de pesos para cada uno de los documentos.  Esto se hace mediante la funci\'on \verb|calculate\_weights|. Esta funci\'on recibe, como primer argumento, el mismo argumento que recibe la funci\'on \verb|calculate_tfijs| descrita anteriormente, y como segundo argumento, un diccionario que contiene los t\'erminos y sus respectivos $idf_i$. Su funcionamiento es el siguiente:
	
	\begin{itemize}
		\item Crear una lista \verb|vec_docs|, que ser\'a en la que se ir\'an guardando los ``vectores'' de pesos
		\item Por cada elemento que devuelve el llamado a la f\'uncion \verb|calculate_tfijs| (recordemos que cada elemento devuelto es un diccionario con los t\'erminos de un documento $d_j$ y sus respectivos $tf_{i,j}$):
		\begin{itemize}
			\item Crear el diccionario \verb|vec_weights|
			\item Por cada uno de los t\'erminos del diccionario devuelto:
			\begin{itemize}
				\item Calcular su peso y guardar la pareja t\'ermino y peso en el diccionario \verb|vec_weights|
			\end{itemize}
			\item A\~{n}adir \verb|vec_weights| a la lista \verb|vec_docs|
		\end{itemize}
		\item Retornar \verb|vec_docs|.
	\end{itemize}
	
	En este m\'etodo se hace un llamado a \verb|calculate_tfijs| la cual sabemos que tiene complejidad temporal $O(n\cdot m)$ y se itera por los $n$ elementos que esta devuelve y por cada uno se recorre una cantidad $O(m)$ de t\'erminos. Por tanto la complejidad temporal de esta funci\'on es $O(n\cdot m)$.
	
	\subsection{Preprocesamiento de las consultas}
	
	El preprocesamiento de las consultas se hace de una forma parecida al preprocesamiento de los documentos. El c\'odigo referente a este apartado se encuentra en el archivo ``queries\_preprocess''.
	
	La primera funci\'on ``query\_preprocessing'' se utiliza para preprocesar una sola consulta. Recibe el texto de una consulta y el diccionario \verb|idfs| que contiene a los t\'erminos con sus respectivos valores de $idf$. Una consulta se procesa de la misma manera que un documento, lo que con un paso extra: luego de que un token haya sido completamente procesado comprobar si pertenece al diccionario \verb|idfs|, pues solo interesan las palabras que pertenezcan al menos a un documento de la colecci\'on. Este m\'etodo devuelve un diccionario que los t\'erminos de la consulta y sus respectivas frecuencias, adem\'as de un entero que indica la frecuencia m\'axima de un t\'ermino en la consulta. Su complejidad temporal es $O(m)$, siendo $m$ la cantidad de palabras de la consulta. 
	
	El siguiente m\'etodo ``queries\_preprocessing'' se utiliza para el preprocesamiento de las consultas de la colecci\'on. Devuelve de forma perezosa el diccionario y el entero explicados anteriormente para cada una de las consultas en la colecci\'on. Su complejidad temporal es $O(n\cdot m)$ siendo $n$ el n\'umero de consultas y $m$ la cantidad de tokens de la consulta con mayor cantidad de tokens.
	
	El \'ultimo m\'etodo ``cran\_recovered\_documents'' se utiliza para obtener, de la relaci\'on de relevancia entre consultas y documentos de la colecci\'on, la cantidad de documentos relevantes para cada una de las consultas.
	
	\section{Representaci\'on de las consultas}
	
	El c\'odigo para calcular los pesos de las consultas se encuentra en el archivo ``queries\_representation.py''. En este se haya la funci\'on ``calculate\_weigths\_queries'' que recibe un valor para $a$, el diccionario \verb|idfs| y un objeto iterable, donde cada elemento de este es una tupla $<$diccionario,entero$>$, tal que el diccionario contiene los t\'erminos de una consulta y su frecuencia en la misma y el entero indica la frecuancia m\'axima de un t\'ermino en la consulta. Esta devuelve una lista con la representaci\'on en pesos de las consultas pasadas en el objeto iterable. La definici\'on de la funci\'on es muy similar a la utilizada para calcular los pesos de los documentos; el \'unico cambio es el de utilizar adem\'as la constante $a$ de suavizado para el c\'alculo de los pesos de una consulta. La complejidad temporal es $O(n\cdot m)$.
	
	Ejecutando este archivo se guarda en un archivo .json los pesos de las consultas de la colecci\'on y en otro la cantidad de documentos relevantes para cada consulta.
	
	\subsection{Similitud entre documentos y consultas}
	
	El c\'odigo relacionado  a este apartado se encuentra en el archivo ``similarity.py''. La primera funci\'on que se aprecia es \verb|sim_doc_query|. Esta recibe los ``vectores'' de pesos de un documento y una consulta (que recordemos son diccionarios) y devuelve un n\'umero que representa la similitud entre el documento y la consulta. La forma de proceder es la siguiente:
	
	\begin{itemize}
		\item Si el documento o la consulta no tiene t\'erminos entonces la similitud es 0.
		
		\item Se calcula $\overrightarrow{d_j}\cdot\overrightarrow{q}$ utilizando solo los t\'erminos que tienen en com\'un ambos
		
		\item Se calcula $|\overrightarrow{d_j}|$ con todos los t\'erminos de $d_J$
		
		\item Se calcula $|\overrightarrow{q}|$ con todos los t\'erminos de $q$
		
		\item Se retorna $\frac{\overrightarrow{d_j}\cdot\overrightarrow{q}}{|\overrightarrow{d_j}|\cdot|\overrightarrow{q}|}$
	\end{itemize}
	
	De forma evidente se aprecia que la complejidad temporal de esta funci\'on es $O(a + b)$, siendo $a$ la cantidad de t\'erminos del documento y $b$ la cantidad de t\'erminos de la consulta.
		
	La pr\'oxima funci\'on es \verb|sim_docs_query| la cual recibe una lista con los vectores de pesos de una colecci\'on de documentos, un vector de pesos de una consulta y un flotante \verb|m| que indica la similitud m\'inima admisible entre documento y consulta. Esta funci\'on devuelve una lista de los documentos(o mejor dicho los id de dichos documentos) m\'as similares a la consulta, ordenados de mayor a menor similitud. Su funcionamiento es el siguiente:
	
	\begin{itemize}
		\item Crear una lista \verb|heap| que utilizaremos como heap, en el cual los documentos estar\'an ordenados parcialmente de menor a mayor similitud.
		\item Por cada uno de los documentos:
		\begin{itemize}
			\item Calcular la similitud entre el documento y la consulta
			\item Si la similitud es mayor o igual que \verb|m| empujar el documento en el heap:
		\end{itemize}
		\item Crear la lista \verb|result| de cardinalidad \verb|m|
		\item Mientras el heap no este vac\'io:
		\begin{itemize}
			\item Extraer el primer elemento del heap y colocar el documento en la posici\'on m\'as a la derecha de \verb|result| a la que a\'un no se le ha asignado un documento.
		\end{itemize}
		\item Retornar la lista \verb|result|
	\end{itemize}
	
	Obs\'ervese que dada la implementaci\'on se garantiza que los documentos m\'as similares a la consulta se guarden en el heap. Y que la lista \verb|result| es un ranking de los documentos del heap ordenados de mayor a menor similitud con la consulta. Las operaciones sobre el heap son $O(\log n)$,siendo $n$ el n\'umero de documentos; y el c\'alculo de similitud ya vimos que es $O(a + b)$. Por tanto la complejidad temporal de guardar los documentos en el heap es $O(n\cdot(a+b+\log n))$. Pasar los documentos de la lista al heap tiene complejidad $O(n\log n)$.  Por tanto la complejidad temporal de esta funci\'on es $O(n\cdot(a+b+\log n))$.
	
	La \'ultima funci\'on \verb|sim_docs_queries| recibe una lista de vectores de pesos de documentos, una lista de vectores de pesos de consultas y una lista de n\'umeros fraccionarios tal que la el elemento en la posici\'on $i$ indica la similitud m\'inima admisible para la consulta $i$. Devuelve una lista donde cada elemento $i$ de dicha lista es una lista con los documentos recuperados para la consulta $i$. Est\'a funci\'on hace un llamado a la funci\'on \verb|sim_docs_query| por cada una de las consultas, por tanto siendo $k$ el n\'umero de consultas llegamos a la conclusi\'on que el m\'etodo tiene una complejidad temporal $O(k\cdot n\cdot(a+b+\log n))$.
	
	\subsection{Interfaz gr\'afica}
	
	Como la interfaz gr\'afica no es objetivo de la asignatura solo vamos a explicar el funcionamiento de la misma, solo decir que para su implementaci\'on se utiliz\'o la librer\'ia \verb|tkinter|. El c\'odigo referente a la misma se encuentra en el archivo ``user.py'' y para acceder a se ejecuta dicho archivo.
	
	Al iniciar la aplicaci\'on, esta luce como muestra la Fig. 1:
	
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width =12.0cm]{inicio.png}
			\caption[Fig1]{Inicio de la aplicaci\'on}		
		\end{center}
	\end{figure}
	
	Para realizar una consulta la escribimos en la barra que aparece debajo del cartel de bienvenida y presionamos el bot\'on ``Buscar''. Al hacer esto la aplicaci\'on mostrar\'a los 10 documentos m\'as similares a la consulta realizada. Por ejemplo veamos, en la Fig 2, que se obtiene al realizar la primera consulta de la colecci\'on.
	
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width =12.0cm]{consulta.png}	
			\caption[Fig2]{Ejemplo de consulta}	
		\end{center}
	\end{figure}
	
	En la Fig. 2 vemos el documento m\'as similar a la consulta, para ver el resto de documentos utilizamos la \textit{scrollbar} situada a la derecha.
	
	Ahora obs\'ervese que a la derecha del documento aparece la pregunta de si >Ha sido \'util el documento? Y a la derecha de esta dos botones: ``S\'i'' y ``No''. El usuario al presionar uno de estos botones indica si el documento es relevante o no. Esta informaci\'on luego se utiliza para el proceso de retroalimentaci\'on.
	
	Por \'ultimo se observa tambi\'en el bot\'on ``Reconsultar'' que este solo aparece una vez (a la derecha del documento m\'as similar) y al presionar en \'el se indica que se rehaga la consulta con la informaci\'on de relevancia e irrelevancia de los documentos. O sea ocurre un proceso de retroalimentaci\'on para obtener mejores resultados.
	
	\section{An\'alisis de los resultados del sistema}
	
	
	\section{Conclusiones}
	
	
	
	\begin{thebibliography}{1}
		
	\end{thebibliography}

\end{document}